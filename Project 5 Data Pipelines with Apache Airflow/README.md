<!-- PROJECT LOGO -->
<br />

<p align="center">
 </a>
 <h1 align="center">Project 4 Data Lake</h1>
 <p align="center">
  Udacity Nanodegree
  <br />
  <a href=https://github.com/BinariesGoalls/Udacity-Data-Engineering-Nanodegree><strong>Explore the repositoryÂ»</strong></a>
  <br />
  <br />
 </p>

</p>


<!-- ABOUT THE PROJECT -->

## About The Project

In this project I learned how to build a data lake and an ETL pipeline in Spark that loads data from S3, processes the data into analytics tables, and loads them back in parquet files into S3.

### Project Description

A music streaming startup, Sparkify, has grown their user base and song database even more and want to move their data warehouse to a data lake. Their data resides in S3, in a directory of JSON logs on user activity on the app, as well as a directory with JSON metadata on the songs in their app.

As their data engineer, you are tasked with building an ETL pipeline that extracts their data from S3, processes them using Spark, and loads the data back into S3 as a set of dimensional tables. This will allow their analytics team to continue finding insights in what songs their users are listening to.

### Tools Used

* Python
* AWS

### Datasets
#### Song Dataset

This first dataset is a subset of [Million Song Dataset](http://millionsongdataset.com/). Each file in the dataset is in JSON format and contains meta-data about a song and the artist of that song. The dataset is hosted at S3 bucket `s3://udacity-dend/song_data`.

Sample :

```
{"num_songs": 1, "artist_id": "ARJIE2Y1187B994AB7", "artist_latitude": null, "artist_longitude": null, "artist_location": "", "artist_name": "Line Renaud", "song_id": "SOUPIRU12A6D4FA1E1", "title": "Der Kleine Dompfaff", "duration": 152.92036, "year": 0}
```

#### Log Dataset

The second dataset is generated by [Event Simulator](https://github.com/Interana/eventsim). These log files in JSON format simulate activity logs from a music streaming application based on specified configurations. The dataset is hosted at S3 bucket `s3://udacity-dend/log_data`.

Sample :

```
{"artist": null, "auth": "Logged In", "firstName": "Walter", "gender": "M", "itemInSession": 0, "lastName": "Frye", "length": null, "level": "free", "location": "San Francisco-Oakland-Hayward, CA", "method": "GET","page": "Home", "registration": 1540919166796.0, "sessionId": 38, "song": null, "status": 200, "ts": 1541105830796, "userAgent": "\"Mozilla\/5.0 (Macintosh; Intel Mac OS X 10_9_4) AppleWebKit\/537.36 (KHTML, like Gecko) Chrome\/36.0.1985.143 Safari\/537.36\"", "userId": "39"}
```


## Data Modeling

### ERD

In this case I knew before-hand that the data types are structured (JSONs) and the amount of data we need to analyze is not big enough to require big data related solutions.

For that reason I used the Star Schema for data modeling in this ETL pipeline, there is two stging_tbles containing all the files information, one fact table containing all the metrics associated to each event songplays, and four dimensions tables, containing complementary informations about songs, artists, users and time, each with a primary key that is being referenced from the fact table. 

The data stored on S3 buckets is extracted and processed using Spark, and is then inserted into the fact and dimensional tables. This tables are stored back to S3 in parquet files, organized for optimal performance.

You can see an Entity Relationship Diagram (ERD) of the built data model below:

![database](./images/Project%203%20tables%20ERD.png)

## Project structure

Files:

| File / Folder |                         Description                          |
| :-----------: | :----------------------------------------------------------: |
|     data      | Folder at the root of the project, where smaller version of data are stored |
|    images     |  Folder at the root of the project, where images are stored  |
|    etl.py     | Loads and processes the data from S3 and stores them back to S3 |
|    dl.cfg     |              Sample configuration file for AWS               |
|    README     |                         Readme file                          |



## How to Run

Clone the repository into a local machine using

```sh
git clone https://github.com/BinariesGoalls/Udacity-Data-Engineering-Nanodegree
```

### Prerequisites

These are the tools necessaries to run the program.

* python 3.7
* AWS account

### Steps

Follow the steps to extract and load the data into the data model.

1. Navigate to `Project 4 Data Lake on AWS` folder

2. Edit the `dl.cfg` configuration file and fill in the AWS Access Key and Secret Key fields

3. Run the S3 bucketcreation process by 

   ```python
   python crete_bucket.py
   ```

4. Run ETL process by 

   ```python
   python etl.py
   ```

   This will execute the commands corresponding to loading data from S3, processing it using Spark and store them back to S3 in parquet files.

5.  The stored files can be explored in AWS S3.


<!-- CONTACT -->

## Contact

Alisson lima - ali2slima10@gmail.com

Linkedin: [https://www.linkedin.com/in/binariesgoalls/](https://www.linkedin.com/in/binariesgoalls/)
